# -*- coding: utf-8 -*-
"""Praetorian_ML_Challenge_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SujHBm-OdDhzRo9bIaQ3BBncSI2pPrD3
"""

import requests
import logging
import base64
import time

logging.basicConfig(level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class Server(object):
    url = 'https://mlb.praetorian.com'
    log = logging.getLogger(__name__)

    def __init__(self):
        self.session = requests.session()
        self.binary  = None
        self.hash    = None
        self.wins    = 0
        self.targets = []

    def _request(self, route, method='get', data=None):
        while True:
            try:
                if method == 'get':
                    r = self.session.get(self.url + route)
                else:
                    r = self.session.post(self.url + route, data=data)
                if r.status_code == 429:
                    raise Exception('Rate Limit Exception')
                if r.status_code == 500:
                    raise Exception('Unknown Server Exception')

                return r.json()
            except Exception as e:
                self.log.error(e)
                self.log.info('Waiting 60 seconds before next request')
                time.sleep(60)

    def get(self):
        r = self._request("/challenge")
        self.targets = r.get('target', [])
        self.binary  = base64.b64decode(r.get('binary', ''))
        return r

    def post(self, target):
        r = self._request("/solve", method="post", data={"target": target})
        self.wins = r.get('correct', 0)
        self.hash = r.get('hash', self.hash)
        self.ans  = r.get('target', 'unknown')
        return r

import pandas as pd
df2=pd.read_csv('praetorian_train.csv')
hex_train = df2.iloc[:,0]   #get data from praetorian_train.csv which has 13000 rows hex_values,target_values
target_train = df2.iloc[:,1]

from sklearn.feature_extraction.text import CountVectorizer

#performs frequency analysis on the input data and constructs a vector of frequencies for each input
vec_opts = {
    "ngram_range": (1, 4),  # allow n-grams of 1-4 words in length (32-bits)
    "analyzer": "word",     # analyze hex words
    "token_pattern": "..",  # treat two characters as a word (e.g. 4b)
    "min_df": 0.001,        # value selected after trial and error for more features 
}
v = CountVectorizer(**vec_opts)
X = v.fit_transform(hex_train, target_train)

for feature, freq in zip(v.inverse_transform(X)[0], X.A[0]):
    print("'%s' : %s" % (feature, freq))

from sklearn.feature_extraction.text import TfidfTransformer
#normalize our data with respect to the corpus of documents using tfâ€“idf
idf_opts = {"use_idf": True}
idf = TfidfTransformer(**idf_opts)

# perform the idf transform
X = idf.fit_transform(X)
print(X)

from sklearn.pipeline import Pipeline
#chain transforms together in our pipeline
pipeline = Pipeline([
    ('vec',   CountVectorizer(**vec_opts)),
    ('idf',  TfidfTransformer(**idf_opts)),
])

X = pipeline.fit_transform(hex_train, target_train)
print(X)

from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
#here we use multinomial naive bayes classifier to get predictions as it is suitable for classification with discrete features as well as fractions from TF-IDF 
clf = MultinomialNB().fit(X,target_train)
predictions = clf.predict(X)
print(predictions)

from sklearn.metrics import classification_report
#here we calculate multi-class classification to get related evaluation metrics
print (classification_report(target_train, predictions))

'''
        #Create training dataset
        data_train=[]
        target_train=[]
        hex_train=[]
        for _ in range(13000):
            # query the /challenge endpoint
            input_dict=s.get()

            # choose a random target and /solve
            target = random.choice(s.targets)
            output_arch=s.post(target)

            data_train.append(binascii.a2b_base64(input_dict.get('binary')))  #get data from server and convert data to binary format for training
          
            target_train.append(output_arch.get('target')) #store target architecture for training      

        hex_train = [binascii.hexlify(e) for e in data_train]  
        df1 = pd.DataFrame({'hex_values':hex_train,'target_values':target_train})  
        df1.to_csv('/content/sample_data/praetorian_train.csv', index=False, header=True)
    '''

if __name__ == "__main__":
    import random,binascii
    import numpy as np

    # create the server object
    s = Server()

    for _ in range(600):
        
        input_dict=s.get()

        data_test=binascii.a2b_base64(input_dict.get('binary')) 
  
        hex_test=binascii.hexlify(s.binary) #convert data from binary to hex format for testing


        x_test = pipeline.transform([hex_test]) #call our pipeline with TF-IDF and Countvectorizer
  
        target = clf.predict(x_test)  
           
        target_to_string=target[0] 

        s.post(target_to_string)

        s.log.info("Guess:[{: >9}]   Answer:[{: >9}]   Wins:[{: >3}]".format(target_to_string, s.ans, s.wins))

        # 500 consecutive correct answers are required to win
        if s.hash:
            s.log.info("You win! {}".format(s.hash))

